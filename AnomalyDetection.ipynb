{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install --upgrade tensorflow==1.15.0"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "!pip install --upgrade systemml"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import systemml"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import tensorflow\nprint(tensorflow.__version__)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Import all dependencies "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import mean_squared_error, mean_absolute_error\nfrom tensorflow.keras.callbacks import Callback\n\nfrom tensorflow import keras"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import numpy as np\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n#from sklearn.metrics import mean_squared_error\n#from keras.models import Sequential\n#from keras.layers import Dense, Dropout\n#from keras.layers import LSTM\n#from keras.callbacks import Callback\n#from keras.models import Sequential\n#from keras.layers import LSTM, Dense, Activation\nimport pickle\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport sys\nfrom queue import Queue\nimport pandas as pd\nimport json\n%matplotlib inline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Load data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Files for training are sampled from the lorenz attractor model implemented in NodeRED. https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-2/"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#!rm watsoniotp.*\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.healthy.phase_aligned.pickle\n!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/lorenzattractor/watsoniotp.broken.phase_aligned.pickle"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!mv watsoniotp.healthy.phase_aligned.pickle watsoniotp.healthy.pickle\n!mv watsoniotp.broken.phase_aligned.pickle watsoniotp.broken.pickle"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_healthy = pickle.load(open('watsoniotp.healthy.pickle', 'rb'), encoding='latin1')\ndata_broken = pickle.load(open('watsoniotp.broken.pickle', 'rb'), encoding='latin1')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_healthy.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "> 3 vibration sensor axes and 3000 samples"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# I) Anomaly detection with supervised classification\n\n> Anomaly detection of data from **Lorenz attractor model**\n\nAssignment from IBM's Coursera course **Applied AI with DeepLearning** as part of the [**Advanced Data Science Specialization**](https://www.coursera.org/specializations/advanced-data-science-ibm#courses).\n\nCan construct a simple feed forward neural net, 3 dense layers, trained on healthy and broken samples.\nIf we were to run into a problem of unsupervised anomaly detection, best bet would be to use an autoencoder and train it with an LSTM on what we believe to be non-anonmalous data.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Import keras tuner to optimize model hyperparameters"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "!pip install keras-tuner"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1 - Visualize data "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig = plt.figure()\nax = fig.gca(projection='3d')\n\nax.plot(data_healthy[:,0], data_healthy[:,1], data_healthy[:,2],lw=0.5)\nax.set_xlabel(\"X Axis\")\nax.set_ylabel(\"Y Axis\")\nax.set_zlabel(\"Z Axis\")\nax.set_title(\"Lorenz Attractor for Healthy Data\");"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "data_healthy_df = pd.DataFrame(data_healthy)\n\ndata_healthy_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Then for the broken one"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig = plt.figure()\nax = fig.gca(projection='3d')\n\nax.plot(data_broken[:,0], data_broken[:,1], data_broken[:,2],lw=0.5)\nax.set_xlabel(\"X Axis\")\nax.set_ylabel(\"Y Axis\")\nax.set_zlabel(\"Z Axis\")\nax.set_title(\"Lorenz Attractor for Broken Data\");"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2 - Transform data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Fast Fourier Transform"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_healthy_fft = np.fft.fft(data_healthy).real # sine decomposition\ndata_broken_fft = np.fft.fft(data_broken).real "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(data_healthy_fft.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(data_healthy_fft)\nax.plot(range(0,size), data_healthy_fft[:,0].real, '-', color=(0,0.5,1), animated = True, linewidth=1, label=['sample 1'])\nax.plot(range(0,size), data_healthy_fft[:,1].real, '-', color=(1,0,0.5), animated = True, linewidth=1, label=['sample 2'])\nax.plot(range(0,size), data_healthy_fft[:,2].real, '-', color=(1,0.5,0), animated = True, linewidth=1, label=['sample 3'])\nax.set_xlabel('Frequency')\nax.set_ylabel('Magnitude')\nax.set_title('Frequency spectrum for healthy data')\nax.legend()\nNone"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "> Samples 2 and 3 (second and third axes) are overlayed"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(data_healthy_fft)\nax.plot(range(0,size), data_broken_fft[:,0].real, '-', color=(0,0.5,1), animated = True, linewidth=1, label=['sample 1'])\nax.plot(range(0,size), data_broken_fft[:,1].real, '-', color=(1,0,0.5), animated = True, linewidth=1, label=['sample 2'])\nax.plot(range(0,size), data_broken_fft[:,2].real, '-', color=(1,0.5,0), animated = True, linewidth=1, label=['sample 3'])\nax.set_xlabel('Frequency')\nax.set_ylabel('Magnitude')\nax.set_title('Frequency spectrum for broken data')\nax.legend()\nNone"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 3 - Scale data for neural network"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def scaleData(data):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    return scaler.fit_transform(data)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_healthy_scaled = scaleData(data_healthy_fft)\ndata_broken_scaled = scaleData(data_broken_fft)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Reshape to have three samples (rows, training example) and 3000 features (columns, features). "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_healthy_scaled = data_healthy_scaled.T\ndata_broken_scaled = data_broken_scaled.T"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_broken_scaled.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#data_healthy_scaled.reshape(3, 3000)\n#data_broken_scaled.reshape(3, 3000)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 4 - Model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# callback to keep track of losses for keras\n\nclass LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        sys.stdout.write(str(logs.get('loss'))+str(', '))\n        sys.stdout.flush()\n        self.losses.append(logs.get('loss'))\n        \nlr = LossHistory()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### use kerastuner for hyperparameter tuning"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from kerastuner import HyperModel\nfrom tensorflow.keras import optimizers\n\nclass FFHyperModel(HyperModel):\n    def __init__(self, input_shape, num_classes):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n    \n    def build(self, hp):\n        model = Sequential()\n        model.add(\n            Dense(\n                units=hp.Int(\n                    'units_l1',\n                    min_value=32,\n                    max_value=480,\n                    step=32,\n                    default=192\n                ),\n                input_shape=self.input_shape,\n                activation='relu'\n            )\n        )\n        model.add(\n            Dense(\n                units=hp.Int(\n                    'units_l2',\n                    min_value=32,\n                    max_value=480,\n                    step=32,\n                    default=96\n                ),\n                activation='relu'\n            )        \n        )\n        model.add(Dense(self.num_classes, activation='sigmoid'))\n        \n        model.compile(\n            optimizer=optimizers.SGD(\n                hp.Float(\n                    'learning_rate',\n                    min_value=1e-4,\n                    max_value=1e-1,\n                    sampling='LOG',\n                    default=1e-2\n                ),\n                clipnorm=1.),\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        return model\n                  "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "dim = 3000\nsamples = 3\nnum_classes = 1"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# supervised, have to add labels (0-broken, 1-healthy) to training data before we train\n\nlabel_healthy = np.repeat(1,3)\nlabel_healthy.shape = (3,1)\nlabel_broken = np.repeat(0,3)\nlabel_broken.shape = (3,1)\n\ntrain_healthy = np.hstack((data_healthy_scaled,label_healthy))\ntrain_broken = np.hstack((data_broken_scaled,label_broken))\ntrain_both = np.vstack((train_healthy,train_broken))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "hypermodel= FFHyperModel(input_shape=(dim,), num_classes=num_classes)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "hypermodel.input_shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from kerastuner.tuners import Hyperband\n\n# Hyperband : optimized version of random search, uses early stopping\n\nepochs=2500\ntuner=Hyperband(hypermodel, max_epochs=epochs, objective='loss', executions_per_trial=2, \n                  directory='hyperband', project_name='supervised_anomaly')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "tuner.search_space_summary()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "features = train_both[:,:3000] \nlabels = train_both[:,3000:] "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# same as model.fit\nepochs = 2500\n\ntuner.search(features, labels, epochs=epochs)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "tuner.results_summary()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "best_model = tuner.get_best_models(num_models=1)[0]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "best_model.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "loss, accuracy = best_model.evaluate(features, labels)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(f'Loss: {loss}, Accuracy: {accuracy}')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pred_healthy = best_model.predict(data_healthy_scaled)\npred_broken = best_model.predict(data_healthy_scaled)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pred_healthy"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pred_broken"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### without the tuner"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "number_of_neurons_layer1 = 200\nnumber_of_neurons_layer2 = 100\nnumber_of_neurons_layer3 = 1 #output: 0,1\nnumber_of_epochs = 2500"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from tensorflow.keras import optimizers\nsgd = optimizers.SGD(lr=0.01, clipnorm=1.)\n\nmodel = Sequential()\nmodel.add(Dense(number_of_neurons_layer1, input_shape=(dim, ), activation='relu'))\nmodel.add(Dense(number_of_neurons_layer2, activation='relu'))\nmodel.add(Dense(number_of_neurons_layer3, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\n\ndef train(data,label): \n    model.fit(data, label, epochs=number_of_epochs, batch_size=72, validation_data=(data, label), verbose=False, shuffle=True, callbacks=[lr])\n    \n    # ideally, would split data for validation set, but since we only have three examples of each class limited options\n\ndef score(data):\n    return model.predict(data)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "train(features,labels)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(lr.losses)\nax.plot(range(0,size), lr.losses, '-', color=(0,0.5,1), animated = True, linewidth=1)\nax.set_ylabel('Loss, binary cross entropy')\nax.set_xlabel('Epochs');"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(lr.losses)\nax.plot(range(0,size), lr.losses, '-', color=(0,0.5,1), animated = True, linewidth=1)\nax.set_ylabel('Loss, binary cross entropy')\nax.set_xlabel('Epochs')\nax.set_ylim(0,0.01);"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# ideally would split for test data set as well\n\nscore(data_healthy_scaled)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "score(data_broken_scaled)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Anomaly detection with unsupervised classification\nUsing an LSTM autoencoder trained on healthy data, and setting an error threshold above which the data will be classified as an anomaly (i.e. can't be reconstructed with the autoencoder)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# healthy sensor data \n\nfig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(data_healthy)\nax.plot(range(0,size), data_healthy[:,0], '-', color=(0,0.5,1), linewidth=1, label='sensor 1')\nax.plot(range(0,size), data_healthy[:,1], '-', color=(1,0,0.5), linewidth=1, label='sensor 2')\nax.plot(range(0,size), data_healthy[:,2], '-', color=(1,0.5,0), linewidth=1, label='sensor 3')\n#ax.set_xlabel('Frequency')\n#ax.set_ylabel('Magnitude')\nax.set_title('Healthy data')\nax.legend()\nNone"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# frequency domain \n\nfig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(data_healthy_fft)\nax.plot(range(0,size), data_healthy_fft[:,0].real, '-', color=(0,0.5,1), linewidth=1, label='X')\nax.plot(range(0,size), data_healthy_fft[:,1].real, '-', color=(1,0,0.5), linewidth=1, label='Y')\nax.plot(range(0,size), data_healthy_fft[:,2].real, '-', color=(1,0.5,0), linewidth=1, label='Z')\nax.set_xlabel('Frequency')\nax.set_ylabel('Magnitude')\nax.set_title('Frequency spectrum for healthy data')\nax.legend()\nNone"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Make sure data is scaled (MinMax scaler was run for part I) and reshape for LSTM.\n\n> LSTM cells expect a 3 dimensional tensor of the form [data samples, time steps, features]. Here, each sample input into the LSTM network represents one step in time and contains 3 features \u2014 the sensor readings for the three axes at that time step."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_healthy.shape[0]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_scaled = scaleData(data_healthy)\ndata_shaped = data_scaled.reshape(data_scaled.shape[0], 1, data_scaled.shape[1])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_shaped.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "data_broken_scaled = scaleData(data_broken)\ndata_broken_shaped = data_broken_scaled.reshape(data_broken_scaled.shape[0], 1, data_broken_scaled.shape[1])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "units_l1, units_l2 = 16, 4"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# use functional Keras API\n# set return_sequences to True to stack LSTM layers (except for repeat vector)\n\nfrom tensorflow.keras.layers import TimeDistributed, RepeatVector\n\ninputs = Input(\n    shape=(\n        data_shaped.shape[1], data_shaped.shape[2]\n    )\n)\n\nlayer_1 = LSTM(\n    units_l1, \n    activation='relu', \n    return_sequences=True\n)(inputs)\n\nlayer_2 = LSTM(\n    units_l2,\n    activation='relu',\n    return_sequences=False\n)(layer_1)\n\nrep = RepeatVector(\n    data_shaped.shape[1])(layer_2)\n\nlayer_3 = LSTM(\n    units_l2,\n    activation='relu',\n    return_sequences=True\n)(rep)\n\nlayer_4 = LSTM(\n    units_l1,\n    activation='relu',\n    return_sequences=True\n)(layer_3)\n\noutput = TimeDistributed(\n    Dense(data_shaped.shape[2])\n)(layer_4)\n\nmodel = Model(inputs=inputs, outputs=output)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model.compile(optimizer='adam', loss='mae')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "EPOCHS=250\nBATCH_SIZE=72\n\nmodel.fit(data_shaped, data_shaped, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[lr])\n    "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\nsize = len(lr.losses)\nax.plot(range(0,size), lr.losses, '-', color=(0,0.5,1), animated = True, linewidth=1)\nax.set_ylabel('Loss, binary cross entropy')\nax.set_xlabel('Epochs');"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pred = model.predict(data_shaped)\n\n# reshape to 3000,3\npred = pred.reshape(pred.shape[0], pred.shape[2])\n\n# compute loss\nloss_mae = np.mean(np.abs(pred - data_scaled), axis=1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import seaborn as sns\n\nsns.distplot(loss_mae, bins=30);"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "> use 0.027 as threshold"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# test with broken data\n\npred_broken = model.predict(data_broken_shaped)\npred_broken = pred_broken.reshape(pred_broken.shape[0], pred_broken.shape[2])\n\n# compute loss\nloss_mae_broken = np.mean(np.abs(pred_broken - data_broken_scaled), axis=1)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "threshold = 0.027\n\nanomaly = loss_mae_broken > threshold\nanomaly.sum()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.figure(figsize=(16,4))\nplt.plot(loss_mae)\nplt.hlines(threshold, 0, len(loss_mae_broken), color='r')\nplt.title('Healthy data')\nplt.ylabel('mae losses');"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "anomaly_toplot = anomaly[anomaly==1]*threshold\n\nplt.figure(figsize=(16,4))\nplt.plot(loss_mae_broken, zorder=0)\nplt.hlines(threshold, 0, len(loss_mae_broken), color='r', label='threshold')\nplt.scatter(np.where(anomaly==1), anomaly_toplot, color='r', s=10, label='flagged anomalies')\nplt.title('Broken data, flagged as anomaly')\nplt.legend()\nplt.ylabel('mae losses');"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Using Apache SystemML\n\nCan train a Keras model on Apache Spark with SystemML. SystemML runs on top of Apache Spark, and it can import keras model definitions."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from systemml.mllearn import Keras2DML\n\nEPOCHS=250\nBATCH_SIZE=72\n\nmax_iter = int(EPOCHS*math.ceil(samples/BATCH_SIZE))\n\nmodel_sysml = Keras2DML(\n    spark, \n    model, \n    input_shape=(data_shaped.shape[1], data_shaped.shape[2]), \n    batch_size=BATCH_SIZE, \n    max_iter=max_iter, \n    test_interval=0, \n    display=10)\n\nmodel_sysml.set(perform_one_hot_encoding=False)\nmodel_sysml.set(debug=True)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}