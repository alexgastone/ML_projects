{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Advanced Regression\n\n[See Problem Description & Data on Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)\n\n> Goal: Predict housing prices\n\n> Method: Ensemble learning outperforms baseline models"},{"metadata":{},"cell_type":"markdown","source":"# I) Setup and imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.gridspec as gridspec\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport scipy.stats as stats\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nimport sklearn.linear_model as linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain.head()","execution_count":349,"outputs":[{"output_type":"execute_result","execution_count":349,"data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n\n  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n\n  YrSold  SaleType  SaleCondition  SalePrice  \n0   2008        WD         Normal     208500  \n1   2007        WD         Normal     181500  \n2   2008        WD         Normal     223500  \n3   2006        WD        Abnorml     140000  \n4   2008        WD         Normal     250000  \n\n[5 rows x 81 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>...</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 81 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**train dataset**: 1460 rows x 81 cols\n\n**test dataset**: 1459 rows x 80 cols (no target)"},{"metadata":{},"cell_type":"markdown","source":"# II) EDA: Data observations"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the features are objects"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_percentage(df):\n    \"\"\"\n    :param df: takes a DataFrame(df) as input \n    \n    returns two columns, total missing values and total missing values percentage\"\"\"\n \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().\n                                                             sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2) != 0]\n    \n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nprint('Number of features with missing value in training data:', missing_percentage(train).shape[0]) #print total train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of features with missing value in test data:', missing_percentage(test).shape[0]) #print total test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test dataset has more features with missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"GnBu_d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check whether target variable, SalePrice, is normally distributed\nsns.set_style('darkgrid')\n\nfig, (ax1,ax2) = plt.subplots(2,1) \nax1 = sns.distplot(train.loc[:,'SalePrice'], norm_hist=True, ax = ax1)\nax1.set_title('Distribution of target values')\ns = stats.probplot(train.loc[:,'SalePrice'], plot = ax2)\nax2.get_lines()[0].set_marker('.')\nax2.get_lines()[0].set_fillstyle('none')#markerfacecolor('none')\nax2.get_lines()[0].set_markeredgecolor('gray')\nplt.tight_layout()\nNone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: \" + str(train['SalePrice'].skew()))\nprint(\"Kurtosis: \" + str(train['SalePrice'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Target varaible is right-skewed (positive skew), and there are outliers. Will need to be addressed with transformations later on."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting the correlation of all the features with target variable. See first 10\n(train.corr())[\"SalePrice\"].sort_values(ascending = False)[1:][0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`OverallQual`, `GrLivArea`, `GarageCars`, `GarageArea`, `TotalBsmtSF`,`1stFlrSF`,`FullBath`, `TotRmsAbvGrd`,`YearBuilt`,and `YearRemodAdd` all have correlation with target >0.5"},{"metadata":{},"cell_type":"markdown","source":"Check a few of these feature distributions (ex 4 most correlated) for variance, whether there's outliers... "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axs) = plt.subplots(2,2, figsize=(10,7))\n#sns.scatterplot(train.OverallQual, train.SalePrice, ax=axs[0,0])\nsns.regplot(x=train.OverallQual, y=train.SalePrice, ax=axs[0,0], scatter_kws={\"s\": 20, \"edgecolor\":'white'})\n\n#sns.scatterplot(train.GrLivArea, train.SalePrice, ax=axs[0,1])\nsns.regplot(x=train.GrLivArea, y=train.SalePrice, ax=axs[0,1], scatter_kws={\"s\": 20,\"edgecolor\":'white'})\n\n#sns.scatterplot(train.GarageCars, train.SalePrice, ax=axs[1,0])\nsns.regplot(x=train.GarageCars, y=train.SalePrice, ax=axs[1,0], scatter_kws={\"s\": 20,\"edgecolor\":'white'})\n\n#sns.scatterplot(train.GarageArea, train.SalePrice, ax=axs[1,1])\nsns.regplot(x=train.GarageArea, y=train.SalePrice, ax=axs[1,1], scatter_kws={\"s\": 20,\"edgecolor\":'white'})\nplt.tight_layout()\nNone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot(train.GrLivArea, train.SalePrice, color='darkcyan',scatter_kws={\"s\": 20,\"edgecolor\":'white'});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variability in target variable increases as the value of the features increases, not equal variance. Outliers."},{"metadata":{},"cell_type":"markdown","source":"# III) Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"## 1- transform target data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## trainsforming target variable taking natural log(1+x)\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nfig, (ax1,ax2) = plt.subplots(2,1) \nax1 = sns.distplot(train.loc[:,'SalePrice'], norm_hist=True, ax = ax1)\nax1.set_title('Distribution of target values')\ns = stats.probplot(train.loc[:,'SalePrice'], plot = ax2)\nax2.get_lines()[0].set_marker('.')\nax2.get_lines()[0].set_fillstyle('none')#markerfacecolor('none')\nax2.get_lines()[0].set_markeredgecolor('gray')\nplt.tight_layout()\nNone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot(train.GrLivArea, train.SalePrice, color='darkcyan',scatter_kws={\"s\": 20,\"edgecolor\":'white'});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transforming the target variable helped with the constant variability issue we were seeing before in other features. Now, almost equal variance."},{"metadata":{},"cell_type":"markdown","source":"## 2- check for multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (30,20))\nsns.heatmap(train.corr(), vmax=0.8, square=True, annot=True, center = 0, cmap='coolwarm_r');\nplt.title(\"Heatmap of all the Features\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 10 #number of variables for heatmap\ncols = train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', cmap='coolwarm_r', center=0, annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Heatmap of features most highly correlated with target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear regression requires independent variables to have little or no similar features, so avoid multicollinearity. Since our data does have some level of correlation between certain features, will have to introduce regularization in model to take care of that."},{"metadata":{},"cell_type":"markdown","source":"## 3- Handle missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformations have to be done on both training and test data\nall_data = pd.concat((train, test), sort=False).reset_index(drop = True)\n\n## Dropping the target variable\nall_data.drop(['SalePrice'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some features should be categorical instead of num\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str) \nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n# fill missing with mode, \n# usually, avoid data leakage by taking mode of training only, but here the missing value\n# is in test data, so take mode of test? would keep two datasets separate\n# group_by & transform -> split data into groups based on some criteria, and apply a function to each \n# group independently\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For missing values, usually consider that when more than 15% of data is missing we should delete the corresponding variable. In this case, could pull more from data descriptions though (ex PoolQC with Na means no pool)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xticks(rotation='90')\nsns.barplot(x=missing_percentage(all_data).index, y=missing_percentage(all_data)['Percent'], palette='GnBu_r')\nplt.xlabel('Features')\nplt.ylabel('Percent of missing values')\nplt.title('Percent missing data by feature for all data')\nNone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(10,5))\n\nsns.barplot(x=missing_percentage(train).index, y=missing_percentage(train)['Percent'], ax=axs[0], palette='GnBu_r')\naxs[0].set_xlabel('Features')\naxs[0].set_ylabel('Percent of missing values')\naxs[0].set_title('Percent missing data by feature for train data')\naxs[0].tick_params(rotation=90)\n\nsns.barplot(x=missing_percentage(test).index, y=missing_percentage(test)['Percent'], ax=axs[1], palette='GnBu_r')\naxs[1].set_xlabel('Features')\naxs[1].set_ylabel('Percent of missing values')\nplt.title('Percent missing data by feature for test data')\naxs[1].tick_params(rotation=90)\n\nplt.tight_layout()\nNone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill with None\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\") # no alley access\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n# hestitant about using median of full dataset...\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\n# fill with relevant corresponding data\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['Functional'] = all_data['Functional'].fillna('Typ')\nall_data['Electrical'] = all_data['Electrical'].fillna(\"SBrkr\") \nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(\"TA\") \nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub') \n\n# fill with mode\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(test['Exterior1st'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(test['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(test['SaleType'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage(all_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All missing data has been handled."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4- Transform highly skewed features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# some features are also skewed, let's see the top 10 most skewed and their values\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewed_feats[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(all_data['1stFlrSF']);\nprint('skew: ', skewed_feats['1stFlrSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use Box-Cox transformation for highly skewed features\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n    \nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nskewed_features = high_skew.index\n\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], boxcox_normmax(all_data[feat] + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(all_data['1stFlrSF']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"transformed to normality!"},{"metadata":{},"cell_type":"markdown","source":"## 5- Feature generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n# and total porch sq as well as total bathrooms\nall_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n                               all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] +\n                              all_data['EnclosedPorch'] + all_data['ScreenPorch'] +\n                              all_data['WoodDeckSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.shape\n# added 8 new features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5- Encode categorical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for encoding categorical values, there are a few that do have information in ordering (ordinal)\n# so try label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'OverallCond', 'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['CentralAir'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the other 31 objects, apply one hot encoding\nall_data = pd.get_dummies(all_data, drop_first=True).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6- Handle outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to remove outliers, use winsorization - floor and ceiling applied to outliers (replaces values)\nwins_data = stats.mstats.winsorize(all_data, limits=[0.05, 0.05])\nwins_data = pd.DataFrame(wins_data, columns=all_data.columns)\nwins_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7- retrieve train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\n\nX = all_data[:ntrain]\ny = train['SalePrice']\n\nX_predict = all_data[ntrain:] #test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_predict.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IV) Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Root mean squared (RMSE) using cross validation\nfolds = 10\n\ndef rmse_cv(model):\n    \"\"\" ex with model=LinearRegression()\"\"\"\n    cv = KFold(shuffle=True, random_state=2, n_splits=folds)\n    scores = cross_val_score(model, X,y,cv = cv, scoring = 'neg_mean_squared_error')\n    rmse = np.sqrt(-scores)\n    \n    return rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline models"},{"metadata":{},"cell_type":"markdown","source":"Due to collinearity, we know we have to introduce regularization (Lasso/Ridge/ENet). Try SVR & other tree-based models as well:\n* **Ridge Regression**\n* **Lasso Regression**\n* **ElasticNet Regression**\n* **Support Vector Regression**\n* **Gradient Boosted Regressor** - ensemble of weak predicition models, boost -> train on residuals from error of previous model\n* **LightGBM**\n* **XGBoost** - fast implementation, even more resistant to outliers"},{"metadata":{},"cell_type":"markdown","source":"For info on light GBM and tuning hyperparams, see https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore what range of hyperparams give best scores with CV\n\n#alpha_grid = [8., 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9]\n#l1_ratio_grid = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n#comb = [(a,b) for a in alpha_grid for b in l1_ratio_grid]\n\n#from sklearn.linear_model import Ridge\n\n#results = {}\n#for candidate_alpha in alpha_grid:\n#for c in comb:\n    #ridge_regression = Ridge(alpha=candidate_alpha)\n    #validation_score = rmse_cv(ridge_regression)\n    #results[candidate_alpha] = validation_score\n    #results['{}_{}'.format(c[0], c[1])] = validation_score\n\n\n#f = lambda x: np.mean(x)\n#avg_results = {k: f(v) for k, v in results.items()}\n\n#avg_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#min(avg_results, key=avg_results.get)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# easier, more streamlined way of instantiating a model object with CV and search hyperparams\n# RobustScaler() : Scale features using statistics that are robust to outliers\n\nkfolds = KFold(n_splits=folds, shuffle=True, random_state=42)\n\nalpha_grid = [8., 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9] #Ridge\nalpha_grid2 = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001] #Lasso\nalpha_grid3 = [0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.002, 0.003, 0.004, 0.05] #ENet\nl1_ratio_grid = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n#For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty.\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alpha_grid, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=alpha_grid2, cv=kfolds, max_iter=1e7))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=alpha_grid3, cv=kfolds, \n                                                        max_iter=1e7, l1_ratio=l1_ratio_grid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, \n                                max_features='sqrt', min_samples_leaf=15, min_samples_split=10, \n                                loss='huber', random_state=42)\n# huber loss helps with outlier robustness","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lightgbm = LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.01, n_estimators=5000,\n                         max_bin=200, bagging_fraction=0.75, bagging_freq=5, bagging_seed=7,\n                         feature_fraction=0.2, feature_fraction_seed=7, verbose=-1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460, max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7, colsample_bytree=0.7, objective='reg:linear', \n                       nthread=-1, scale_pos_weight=1, seed=27, reg_alpha=0.00006)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# without cv, calcula\ndef rmse(model, X, y):\n    return np.sqrt(-cross_val_score(model, X, y, cv = kfolds, scoring = 'neg_mean_squared_error'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_score = rmse(lasso, X, y)\nridge_score = rmse(ridge, X, y)\nenet_score = rmse(elasticnet, X, y)\nsvr_score = rmse(svr, X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract avg & std across all cross validated folds\n\nprint('Lasso: mean {:.4f}, std {:.4f}'.format(np.mean(lasso_score), np.std(lasso_score)))\nprint('Ridge: mean {:.4f}, std {:.4f}'.format(np.mean(ridge_score), np.std(ridge_score)))\nprint('ElasticNet: mean {:.4f}, std {:.4f}'.format(np.mean(enet_score), np.std(enet_score)))\nprint('SVR: mean {:.4f}, std {:.4f}'.format(np.mean(svr_score), np.std(svr_score)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"lightgbm_score = rmse(lightgbm, X, y)\nxgboost_score = rmse(xgboost,X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract avg across all cross validated folds\n\nprint('LightGBM: mean {:.4f}, std {:.4f}'.format(np.mean(lightgbm_score), np.std(lightgbm_score)))\nprint('XGBoost: mean {:.4f}, std {:.4f}'.format(np.mean(xgboost_score), np.std(xgboost_score)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GBR can't handle too large values now (ex float32, even float16), so have to convert\nX_int = X.astype(np.int64)\ny_int = y.astype(np.int64)\ngbr_score = rmse(gbr, X_int, y_int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GBR: mean {:.4f}, std {:.4f}'.format(np.mean(gbr_score), np.std(gbr_score)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Try stacking models, with meta learner xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, svr, xgboost, lightgbm),\n                            meta_regressor=xgboost,\n                            use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_score = rmse(stack, np.array(X), np.array(y))\nprint('Stack: mean {:.4f}, std {:.4f}'.format(np.mean(gbr_score), np.std(gbr_score)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Stacked: mean {:.4f}, std {:.4f}'.format(np.mean(stack_score), np.std(stack_score)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = pd.DataFrame([lasso_score, ridge_score, enet_score, svr_score, gbr_score, \n                          lightgbm_score, xgboost_score, stack_score],\n                          index=['lasso', 'ridge', 'enet', 'svr', 'gbr', 'lightbgm', \n                                 'xgb', 'stacked'])#.transpose()\nmean_df = df_scores.mean(axis=1)\nstd_df = df_scores.std(axis=1)\n\nmean_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.scatterplot(x=np.arange(8), y=mean_df, s=40, label='mean score')\nplt.fill_between(np.arange(8), mean_df-std_df, mean_df+std_df, color='grey', alpha=0.2, label='+/- std')\nplt.legend()\nplt.ylabel('score')\ng.set(xticklabels=['0','lasso', 'ridge', 'enet', 'svr', 'gbr', 'lightbgm', 'xgb', 'stacked']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = df_scores.drop('gbr')\nmean_df = df_scores.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.scatterplot(x=np.arange(7), y=mean_df, s=40)\nplt.ylabel('score')\ng.set(xticklabels=['0','lasso', 'ridge', 'enet', 'svr', 'lightbgm', 'xgb', 'stacked']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Start training"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Lasso model')\nlasso_model = lasso.fit(X_train,y_train)\n\nprint('Training Ridge model')\nridge_model = ridge.fit(X_train,y_train)\n\nprint('Training ENet model')\nenet_model = elasticnet.fit(X_train,y_train)\n\nprint('Training SVR model')\nsvr_model = svr.fit(X_train,y_train)\n\nprint('Training XGBoost model')\nxgb_model = xgboost.fit(X_train,y_train)\n\nprint('Training LightGBM model')\nlgb_model = lightgbm.fit(X_train,y_train)\n\nprint('Training Stacked model')\nstack_model = stack.fit(np.array(X_train), np.array(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse_pred(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try predicting training data \n\nprint('lasso rmse: {:.4f}'.format(rmse_pred(y_train, lasso_model.predict(X_train))))\nprint('ridge rmse: {:.4f}'.format(rmse_pred(y_train, ridge_model.predict(X_train))))\nprint('enet rmse: {:.4f}'.format(rmse_pred(y_train, enet_model.predict(X_train))))\nprint('svr rmse: {:.4f}'.format(rmse_pred(y_train, svr_model.predict(X_train))))\nprint('lightgbm rmse: {:.4f}'.format(rmse_pred(y_train, lgb_model.predict(X_train))))\nprint('xgb rmse: {:.4f}'.format(rmse_pred(y_train, xgb_model.predict(X_train))))\nprint('stacked rmse: {:.4f}'.format(rmse_pred(y_train, stack_model.predict(np.array(X_train)))))","execution_count":358,"outputs":[{"output_type":"stream","text":"lasso rmse: 0.0920\nridge rmse: 0.1018\nenet rmse: 0.0925\nsvr rmse: 0.1051\nlightgbm rmse: 0.0627\nxgb rmse: 0.0426\nstacked rmse: 0.0441\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_train_pred = lasso_model.predict(X_train)\nridge_train_pred = ridge_model.predict(X_train)\nenet_train_pred = enet_model.predict(X_train)\nsvr_train_pred = svr_model.predict(X_train)\nlgb_train_pred = lgb_model.predict(X_train)\nxgb_train_pred = xgb_model.predict(X_train)\nstack_train_pred = stack_model.predict(np.array(X_train))","execution_count":357,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After training on (training) data, **XGBoost** performs best, closely followed by the **Stacked** model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# validate with test data\n\nprint('lasso rmse: {:.4f}'.format(rmse_pred(y_test, lasso_model.predict(X_test))))\nprint('ridge rmse: {:.4f}'.format(rmse_pred(y_test, ridge_model.predict(X_test))))\nprint('enet rmse: {:.4f}'.format(rmse_pred(y_test, enet_model.predict(X_test))))\nprint('svr rmse: {:.4f}'.format(rmse_pred(y_test, svr_model.predict(X_test))))\nprint('lightgbm rmse: {:.4f}'.format(rmse_pred(y_test, lgb_model.predict(X_test))))\nprint('xgb rmse: {:.4f}'.format(rmse_pred(y_test, xgb_model.predict(X_test))))\nprint('stacked rmse: {:.4f}'.format(rmse_pred(y_test, stack_model.predict(np.array(X_test)))))","execution_count":359,"outputs":[{"output_type":"stream","text":"lasso rmse: 0.1604\nridge rmse: 0.1456\nenet rmse: 0.1568\nsvr rmse: 0.1479\nlightgbm rmse: 0.1194\nxgb rmse: 0.1199\nstacked rmse: 0.1290\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Generalizes reasonably well to unseen data."},{"metadata":{},"cell_type":"markdown","source":"### Try blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"def blend_predict_train(lasso_w, ridge_w, enet_w, svr_w, lgb_w, xgb_w, stack_w):\n    \"\"\"\"Calculates predicted targets from training data as blend of baseline models,\n    each baseline model's prediction is weighted by its corresponding weight\"\"\"\n    blend_prediction = lasso_w * lasso_train_pred + \\\n                        ridge_w * ridge_train_pred + \\\n                        enet_w * enet_train_pred + \\\n                        svr_w * svr_train_pred + \\\n                        lgb_w * lgb_train_pred + \\\n                        xgb_w * xgb_train_pred +  \\\n                        stack_w * stack_train_pred\n    \n    return blend_prediction","execution_count":361,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find weights through grid search that minimizes rmse on training data\n\nfrom sklearn.model_selection  import ParameterGrid\n\ngrid_range = [0.05,0.1,0.15,0.2,0.25,0.3,0.35]\nparam_grid = {'lasso': grid_range, 'ridge': grid_range, 'enet': grid_range,\n             'svr': grid_range, 'lgb': grid_range, 'xgb': grid_range, 'stack': grid_range}\ngrid = ParameterGrid(param_grid)\npred_score = []\nsub_param_grid = [] \n\nfor params in grid:\n    if params['lasso']+params['ridge']+params['enet']+params['svr']+params['lgb']+params['xgb']+params['stack']==1:\n        pred = blend_predict_train(params['lasso'],params['ridge'], params['enet'], params['svr'], \n                                params['lgb'], params['xgb'], params['stack'])\n        pred_score.append(rmse_pred(y_train, pred))\n        sub_param_grid.append({'lasso': params['lasso'], 'ridge': params['ridge'], \n                               'enet': params['enet'], 'svr': params['svr'], \n                               'lgb': params['lgb'], 'xgb': params['xgb'], \n                               'stack': params['stack']})\n    ","execution_count":368,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_index = np.argmin(pred)\nsub_param_grid[min_index]","execution_count":370,"outputs":[{"output_type":"execute_result","execution_count":370,"data":{"text/plain":"{'lasso': 0.05,\n 'ridge': 0.1,\n 'enet': 0.05,\n 'svr': 0.1,\n 'lgb': 0.15,\n 'xgb': 0.35,\n 'stack': 0.2}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def blend_predict_X(lasso_w, ridge_w, enet_w, svr_w, lgb_w, xgb_w, stack_w, X):\n    \"\"\"\"Calculates predicted targets from training data as blend of baseline models,\n    each baseline model's prediction is weighted by its corresponding weight\"\"\"\n    blend_prediction = lasso_w * lasso_model.predict(X) + \\\n                        ridge_w * ridge_model.predict(X) + \\\n                        enet_w * enet_model.predict(X) + \\\n                        svr_w * svr_model.predict(X) + \\\n                        lgb_w * lgb_model.predict(X) + \\\n                        xgb_w * xgb_model.predict(X) +  \\\n                        stack_w * stack_model.predict(np.array(X))\n    \n    return blend_prediction","execution_count":376,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_predict = blend_predict_X(0.05,0.1,0.05,0.1,0.15,0.35,0.2,X_train)","execution_count":377,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_pred(y_train, blend_predict)","execution_count":378,"outputs":[{"output_type":"execute_result","execution_count":378,"data":{"text/plain":"0.05619651742789181"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_predict = 0.05 * lasso_model.predict(X_train) + \\\n                0.1 * ridge_model.predict(X_train) + \\\n                0.05 * enet_model.predict(X_train) + \\\n                0.1 * svr_model.predict(X_train) + \\\n                0.15 * lgb_model.predict(X_train) + \\\n                0.35 * xgb_model.predict(X_train) +  \\\n                0.2 * stack_model.predict(np.array(X_train)) ","execution_count":371,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data\n\nblend_predict_test = blend_predict_X(0.05,0.1,0.05,0.1,0.15,0.35,0.2,X_test)\n\nrmse_pred(y_test, blend_predict_test)","execution_count":379,"outputs":[{"output_type":"execute_result","execution_count":379,"data":{"text/plain":"0.11929571300501372"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Blending performs slightly better than XGBoost on unseen data (test)."},{"metadata":{},"cell_type":"markdown","source":"## submission for test (unlabelled) data given to us"},{"metadata":{"trusted":true},"cell_type":"code","source":"blend_predict_subm = blend_predict_X(0.05,0.1,0.05,0.1,0.15,0.35,0.2,X_predict)","execution_count":380,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Predict submission')\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_predict_subm))","execution_count":381,"outputs":[{"output_type":"stream","text":"Predict submission\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}